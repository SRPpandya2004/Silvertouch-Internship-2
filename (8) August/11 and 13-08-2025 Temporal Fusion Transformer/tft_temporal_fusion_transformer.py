# -*- coding: utf-8 -*-
"""TFT Temporal Fusion Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QSHnFWtr2Pz3f65a5LuuPrj6AcOLPuFk

# **Second Code**

✅ Cell 1: Install Dependencies
"""

# Install specific versions
!pip install pytorch-lightning==2.4.0 pytorch-forecasting==1.4.0 --quiet

"""✅ Cell 2: Load and Prepare the Data"""

import pandas as pd
import numpy as np
import torch
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("/content/sample_data/dengue data 17-07-2025.csv")  # Change path if needed

# Convert date column
df['week_start_date'] = pd.to_datetime(df['week_start_date'])

# Filter only 'sj' city
# df = df[df['city'] == 'sj'].copy()

# Fill missing values
df.fillna(method='ffill', inplace=True)
df.fillna(method='bfill', inplace=True)

# Extract date features
df['month'] = df['week_start_date'].dt.month
df['day'] = df['week_start_date'].dt.day
df['dayofweek'] = df['week_start_date'].dt.dayofweek
df['year'] = df['week_start_date'].dt.year

# Convert city to categorical code
df['city'] = df['city'].astype('category').cat.codes

# Sort and create continuous time index
df = df.sort_values('week_start_date').reset_index(drop=True)
df['time_idx'] = df.index

df.info()

"""✅ Cell 3: Train/Test Split"""

# 80/20 split
train_size = int(len(df) * 0.8)
train_df = df.iloc[:train_size].copy()
test_df = df.iloc[train_size:].copy()
train_df["time_idx"]

"""✅ Cell 4: Prepare TimeSeriesDataSet"""

from pytorch_forecasting import TimeSeriesDataSet
from pytorch_forecasting.data.encoders import GroupNormalizer
from pytorch_lightning import seed_everything

seed_everything(42)

# Define features
features = [
    'city', 'year', 'ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw',
    'precipitation_amt_mm', 'reanalysis_air_temp_k', 'reanalysis_tdtr_k',
    'station_avg_temp_c', 'station_diur_temp_rng_c', 'station_max_temp_c',
    'station_min_temp_c', 'station_precip_mm', 'month', 'day', 'dayofweek'
]

target = 'total_cases'
max_encoder_length = 30
max_prediction_length = 10
training_cutoff = train_df["time_idx"].max() - max_prediction_length

# Create training dataset
training = TimeSeriesDataSet(
    train_df,#[lambda x: x.time_idx <= training_cutoff],
    time_idx="time_idx",
    target=target,
    group_ids=["city"],
    min_encoder_length=max_encoder_length // 2,
    max_encoder_length=max_encoder_length,
    max_prediction_length=max_prediction_length,
    time_varying_known_reals=["time_idx"] + features,
    time_varying_unknown_reals=[target],
    target_normalizer=GroupNormalizer(groups=["city"]),
    allow_missing_timesteps=True
)

validation = TimeSeriesDataSet.from_dataset(training, test_df, stop_randomization=True)

"""✅ Cell 5: Create DataLoaders"""

batch_size = 64
train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)
val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)

"""
✅ Cell 6: Define and Train TFT Model"""

# ✅ Cell 6: Define and Train TFT Model (consistent Lightning imports)

import lightning.pytorch as L
from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor
from pytorch_forecasting import TemporalFusionTransformer
from pytorch_forecasting.metrics import QuantileLoss
import torch

# (optional) a tiny perf tweak on CPU
torch.set_float32_matmul_precision("medium")

early_stop = EarlyStopping(monitor="val_loss", patience=5, mode="min")
lr_logger = LearningRateMonitor(logging_interval="epoch")

trainer = L.Trainer(
    max_epochs=30,
    gradient_clip_val=0.1,
    callbacks=[early_stop, lr_logger],
    accelerator="auto",   # will use CPU here (GPU not available)
    devices=1,
)

# Build TFT from the *training dataset*, not the dataloader
tft = TemporalFusionTransformer.from_dataset(
    training,
    learning_rate=0.003,
    hidden_size=64,
    attention_head_size=4,
    dropout=0.1,
    hidden_continuous_size=64,
    output_size=7,               # 7 quantiles
    loss=QuantileLoss(),         # required when using quantiles
    log_interval=10,
    reduce_on_plateau_patience=4
)

# Sanity check: should print True
print("Is LightningModule?", isinstance(tft, L.LightningModule))

trainer.fit(
    model=tft,
    train_dataloaders=train_dataloader,
    val_dataloaders=val_dataloader,
)

"""✅ Cell 7: Evaluate the Model"""

best_model_path = trainer.checkpoint_callback.best_model_path
best_model = TemporalFusionTransformer.load_from_checkpoint(best_model_path)

actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])
predictions = best_model.predict(val_dataloader)

y_true = actuals.numpy()
y_pred = predictions.numpy()

mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_true, y_pred)

print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")

"""✅ Cell 8: Plot Actual vs Predicted"""

plt.figure(figsize=(15, 6))
plt.plot(y_true, label='Actual')
plt.plot(y_pred, label='Forecasted', linestyle='--')
plt.title("Actual vs Forecasted Total Cases (City: SJ)")
plt.xlabel("Time Step")
plt.ylabel("Total Cases")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

