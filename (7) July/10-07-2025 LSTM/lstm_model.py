# -*- coding: utf-8 -*-
"""LSTM Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q79bkaFYoya2liQgYIUcrjwrDERr0B5e
"""

# prompt: create a model for lstm

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout

# Define the model parameters
vocab_size = 10000  # Example: size of your vocabulary
embedding_dim = 128 # Example: dimension of word embeddings
lstm_units = 64     # Example: number of LSTM units
max_length = 100    # Example: maximum sequence length

# Create the LSTM model
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))
model.add(LSTM(units=lstm_units, return_sequences=False)) # Set return_sequences=True for stacked LSTMs
model.add(Dropout(0.2)) # Optional: Add dropout for regularization
model.add(Dense(1, activation='sigmoid')) # Example: Output layer for binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Example: binary classification

# Print the model summary
model.summary()

pip install tensorflow pandas numpy matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler

# Load your dataset
data = pd.read_csv('/content/sample_data/milk_production.csv')  # should contain at least one column like 'Close'
data = data[['Production']].values

# Normalize data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

# Create sequences
X, y = [], []
sequence_length = 60
for i in range(sequence_length, len(scaled_data)):
    X.append(scaled_data[i-sequence_length:i])
    y.append(scaled_data[i])

X, y = np.array(X), np.array(y)

model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))
model.add(LSTM(units=50))
model.add(Dense(1))  # Output layer

model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(X, y, epochs=10, batch_size=32)

predicted = model.predict(X)
predicted = scaler.inverse_transform(predicted)

plt.plot(scaler.inverse_transform(y.reshape(-1, 1)), label='Actual')
plt.plot(predicted, label='Predicted')
plt.legend()
plt.show()

"""# **Second Code**"""

# Mount Google Drive (if your data is there)
# from google.colab import drive
# drive.mount('/content/drive')

# Core libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# TensorFlow / Keras
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# For hyperparameter search
!pip install -q keras-tuner
import kerastuner as kt

# Replace with your path
df = pd.read_csv('/content/sample_data/milk_production.csv')

# Check the columns in the dataframe to find the correct date column name
print(df.columns)
df.plot(figsize=(12,4), title='Original Series')
plt.show()
# Once the correct column name is identified, replace 'date' with the correct name
# df = pd.read_csv('/content/sample_data/milk_production.csv', parse_dates=['Date'], index_col='Date')
# df.plot(figsize=(12,4), title='Original Series')
# plt.show()

# 3.1 Scaling
scaler = MinMaxScaler()
values = scaler.fit_transform(df[['Production']])

# 3.2 Sequence builder
def make_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i : i+seq_len])
        y.append(data[i+seq_len])
    return np.array(X), np.array(y)

seq_len = 30
X, y = make_sequences(values, seq_len)

# 3.3 Train/Test split (80/20)
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

def build_baseline():
    model = Sequential([
        LSTM(50, input_shape=(seq_len, 1)),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

model = build_baseline()
es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)

history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=32,
    callbacks=[es, rlr],
    verbose=2
)

def build_advanced(units=64, dropout_rate=0.2, bidirectional=False):
    model = Sequential()
    if bidirectional:
        model.add(Bidirectional(LSTM(units, return_sequences=True), input_shape=(seq_len,1)))
        model.add(Dropout(dropout_rate))
        model.add(Bidirectional(LSTM(units)))
    else:
        model.add(LSTM(units, return_sequences=True, input_shape=(seq_len,1)))
        model.add(Dropout(dropout_rate))
        model.add(LSTM(units))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

adv_model = build_advanced(units=128, dropout_rate=0.3, bidirectional=True)
adv_history = adv_model.fit(
    X_train,y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=32,
    callbacks=[es, rlr],
    verbose=2
)

def model_builder(hp):
    hp_units = hp.Int('units', min_value=32, max_value=256, step=32)
    hp_dropout = hp.Float('dropout', 0.1, 0.5, step=0.1)
    hp_bi = hp.Boolean('bidirectional')

    model = build_advanced(units=hp_units, dropout_rate=hp_dropout, bidirectional=hp_bi)
    return model

tuner = kt.RandomSearch(
    model_builder,
    objective='val_loss',
    max_trials=10,
    directory='lstm_tuner',
    project_name='time_series'
)

tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[es], verbose=1)

best_model = tuner.get_best_models(num_models=1)[0]

# Predict
preds = best_model.predict(X_test)
preds_inv = scaler.inverse_transform(preds)
y_test_inv = scaler.inverse_transform(y_test)

# Metrics
mae = mean_absolute_error(y_test_inv, preds_inv)
rmse = np.sqrt(mean_squared_error(y_test_inv, preds_inv))

print(f'MAE: {mae:.3f}, RMSE: {rmse:.3f}')

# Plot
plt.figure(figsize=(12,4))
plt.plot(df.index[-len(preds_inv):], y_test_inv, label='Actual')
plt.plot(df.index[-len(preds_inv):], preds_inv, label='Predicted')
plt.legend()
plt.show()