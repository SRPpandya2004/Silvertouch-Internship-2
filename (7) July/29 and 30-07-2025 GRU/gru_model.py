# -*- coding: utf-8 -*-
"""GRU Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16qudbZ7GDAOhVOa_fK4VTg-Wi-UyEosk

Cell 1 â€“ Load and Preprocess Data
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv('/content/sample_data/dengue data 17-07-2025.csv')
df['week_start_date'] = pd.to_datetime(df['week_start_date'], dayfirst=True)
df.set_index('week_start_date', inplace=True)

# Focus on San Juan city
df = df[df['city'] == 'sj']
df.drop(['city'], axis=1, inplace=True)

# Fill missing values
df.fillna(method='ffill', inplace=True)

"""Cell 2 â€“ Separate and Scale Features & Target"""

# Scale features (excluding target)
feature_scaler = MinMaxScaler()
scaled_features = feature_scaler.fit_transform(df.drop(['total_cases'], axis=1))

# Scale target separately
target = df['total_cases'].values.reshape(-1, 1)
target_scaler = MinMaxScaler()
scaled_target = target_scaler.fit_transform(target)

"""Cell 3 â€“ Sequence Preparation"""

def create_sequences(features, target, seq_length):
    X, y = [], []
    for i in range(len(features) - seq_length):
        X.append(features[i:i + seq_length])
        y.append(target[i + seq_length])
    return np.array(X), np.array(y)

sequence_length = 10
X, y = create_sequences(scaled_features, scaled_target, sequence_length)

"""Cell 4 â€“ Train-Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

"""Cell 5 â€“ Model Building"""

model = Sequential([
    GRU(units=256, dropout=0.2, recurrent_dropout=0.2,
        activation='tanh', recurrent_activation='sigmoid',
        input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()

"""Cell 6 â€“ Training the Model"""

early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=1
)

"""Cell 7 â€“ Plot Loss Curves


"""

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.show()

"""Cell 8 â€“ Model Evaluation

"""

test_loss, test_mae = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}")

"""Cell 9 â€“ Make Predictions and Inverse Transform"""

# Predict
predicted_scaled = model.predict(X_test)

# Inverse transform predictions and actual values
predicted_cases = target_scaler.inverse_transform(predicted_scaled)
actual_cases = target_scaler.inverse_transform(y_test)

"""Cell 10 â€“ Plot Actual vs Predicted"""

plt.figure(figsize=(12,6))
plt.plot(actual_cases, label='Actual Cases')
plt.plot(predicted_cases, label='Predicted Cases')
plt.title('GRU Prediction on Test Set (Actual Scale)')
plt.xlabel('Time')
plt.ylabel('Total Cases')
plt.legend()
plt.show()

"""# **Second Code**

Cell 1 â€“ Imports & Setup
"""

# Cell 1: Imports and city filtering
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import VarianceThreshold

# Load data
df = pd.read_csv('/content/sample_data/dengue data 17-07-2025.csv')

# Choose one city only (e.g., 'sj' or 'iq')
selected_city = 'sj'  # ðŸ‘ˆ Change to 'iq' for Iquitos
df = df[df['city'] == selected_city].copy()

# Drop unused columns
df.drop(columns=['city', 'week_start_date'], inplace=True)

# Forward fill NaNs
df.fillna(method='ffill', inplace=True)

# --- Visualization ---
print("Data shape after city filtering:", df.shape)
print("City selected:", selected_city)

## Missing data heatmap
# plt.figure(figsize=(12, 8))
# sns.heatmap(df.isnull(), cbar=False, yticklabels=False)
# plt.title("Missing Data Heatmap Before Outlier Removal")
# plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Heatmap Before Outlier Removal")
plt.show()

"""Cell 2: Visualize Outliers Before Removal with Heatmap"""

# plt.figure(figsize=(12, 8))
# sns.heatmap(df.isnull(), cbar=False, yticklabels=False)
# plt.title("Missing Data Heatmap Before Outlier Removal")
# plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Heatmap Before Outlier Removal")
plt.show()

"""Cell 3: Outlier Removal Using IQR & Visualize After Removal"""

def remove_outliers(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    condition = ~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)
    return df[condition]

df_no_outliers = remove_outliers(df)

print(f"Rows before outlier removal: {len(df)}")
print(f"Rows after outlier removal: {len(df_no_outliers)}")

# plt.figure(figsize=(12, 8))
# sns.heatmap(df_no_outliers.isnull(), cbar=False, yticklabels=False)
# plt.title("Missing Data Heatmap After Outlier Removal")
# plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(df_no_outliers.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Heatmap After Outlier Removal")
plt.show()

"""Cell 4: Lag Features Creation & Target/Feature Separation



"""

df = df_no_outliers.copy()

for lag in [1, 2, 3]:
    df[f'total_cases_lag{lag}'] = df['total_cases'].shift(lag)

df.dropna(inplace=True)

target_col = 'total_cases'
features = df.drop(columns=[target_col])

"""Cell 5: Remove Low Variance & High Correlation Features"""

def drop_low_variance(df, thresh=0.01):
    sel = VarianceThreshold(thresh)
    return df[df.columns[sel.fit(df).get_support()]]

def drop_high_corr(df, thresh=0.9):
    corr = df.corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
    to_drop = [col for col in upper.columns if any(upper[col] > thresh)]
    return df.drop(columns=to_drop)

features = drop_low_variance(features)
features = drop_high_corr(features)

features

"""Cell 6: Scale Features and Target"""

feat_scaler = MinMaxScaler()
target_scaler = MinMaxScaler()

X_feat = feat_scaler.fit_transform(features)
y = df[target_col].values.reshape(-1, 1)
y_scaled = target_scaler.fit_transform(y)

print("Feature shape:", X_feat.shape)
print("Target shape:", y_scaled.shape)

"""Cell 7.1: Reshape into Sequences for GRU"""

def create_sequences(X_data, y_data, seq_len):
    X_seq, y_seq = [], []
    for i in range(len(X_data) - seq_len):
        X_seq.append(X_data[i:i+seq_len])
        y_seq.append(y_data[i+seq_len])
    return np.array(X_seq), np.array(y_seq)

sequence_length = 10  # you can tune this
X, y = create_sequences(X_feat, y_scaled, sequence_length)

# Final train-test split
train_size = int(0.8 * len(X))
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]

pip install optuna

""" Cell 8: GRU Model & Optuna Objective"""

import optuna
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dropout, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_squared_error

def create_gru_model(input_shape, trial):
    model = Sequential()
    n_layers = trial.suggest_int('n_layers', 1, 3)

    for i in range(n_layers):
        units = trial.suggest_int(f'units_l{i}', 64, 256, step=32)
        dropout = trial.suggest_float(f'dropout_l{i}', 0.1, 0.5)
        return_seq = i < n_layers - 1

        if i == 0:
            model.add(GRU(units, return_sequences=return_seq, input_shape=input_shape))
        else:
            model.add(GRU(units, return_sequences=return_seq))
        model.add(Dropout(dropout))

    model.add(Dense(1))
    optimizer = trial.suggest_categorical('optimizer', ['adam', 'nadam', 'rmsprop'])
    model.compile(optimizer=optimizer, loss='mse')
    return model

def objective(trial):
    model = create_gru_model((X_train.shape[1], X_train.shape[2]), trial)

    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)

    model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=100,
        batch_size=trial.suggest_categorical('batch_size', [32, 64, 128]),
        callbacks=[early_stop],
        verbose=0
    )

    y_pred_val = model.predict(X_test)
    val_mse = mean_squared_error(y_test, y_pred_val)
    return val_mse

""" Cell 9: Run Optuna Tuning"""

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=30)

print("Best trial:", study.best_trial)
print("Best hyperparameters:", study.best_trial.params)

""" Cell 10: Train Final Model with Best Params"""

best_params = study.best_trial.params

def train_best_gru(params):
    model = Sequential()
    for i in range(params['n_layers']):
        units = params[f'units_l{i}']
        dropout = params[f'dropout_l{i}']
        return_seq = i < params['n_layers'] - 1

        if i == 0:
            model.add(GRU(units, return_sequences=return_seq, input_shape=(X_train.shape[1], X_train.shape[2])))
        else:
            model.add(GRU(units, return_sequences=return_seq))
        model.add(Dropout(dropout))

    model.add(Dense(1))
    model.compile(optimizer=params['optimizer'], loss='mse')

    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)

    model.fit(X_train, y_train,
              validation_split=0.2,
              epochs=100,
              batch_size=params['batch_size'],
              callbacks=[early_stop],
              verbose=1)

    return model

final_model = train_best_gru(best_params)

"""Cell 11: Evaluate Performance (MSE, RMSE)"""

# Predict and inverse transform
y_pred_scaled = final_model.predict(X_test)
y_test_inv = target_scaler.inverse_transform(y_test)
y_pred_inv = target_scaler.inverse_transform(y_pred_scaled)

# Metrics
mse = mean_squared_error(y_test_inv, y_pred_inv)
rmse = np.sqrt(mse)

print(f"Test MSE: {mse:.2f}")
print(f"Test RMSE: {rmse:.2f}")

"""Cell 12: Plot Predicted vs Actual"""

plt.figure(figsize=(12,6))
plt.plot(y_test_inv, label='Actual Cases', linewidth=2)
plt.plot(y_pred_inv, label='Predicted Cases', linewidth=2)
plt.title("GRU Model: Actual vs Predicted Dengue Cases")
plt.xlabel("Time Steps")
plt.ylabel("Total Cases")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""Best trial: FrozenTrial(number=18, state=1, values=[0.02011046394426647], datetime_start=datetime.datetime(2025, 7, 29, 10, 53, 8, 742907), datetime_complete=datetime.datetime(2025, 7, 29, 10, 53, 29, 239496), params={'n_layers': 1, 'units_l0': 192, 'dropout_l0': 0.25124974868498806, 'optimizer': 'adam', 'batch_size': 32}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_layers': IntDistribution(high=3, log=False, low=1, step=1), 'units_l0': IntDistribution(high=256, log=False, low=64, step=32), 'dropout_l0': FloatDistribution(high=0.5, log=False, low=0.1, step=None), 'optimizer': CategoricalDistribution(choices=('adam', 'nadam', 'rmsprop')), 'batch_size': CategoricalDistribution(choices=(32, 64, 128))}, trial_id=18, value=None)
Best hyperparameters: {'n_layers': 1, 'units_l0': 192, 'dropout_l0': 0.25124974868498806, 'optimizer': 'adam', 'batch_size': 32}
"""