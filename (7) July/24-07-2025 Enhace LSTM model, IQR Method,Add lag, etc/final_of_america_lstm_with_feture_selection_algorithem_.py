# -*- coding: utf-8 -*-
"""Final of America LSTM with Feture selection Algorithem .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I9vH62uzWL2IkIUMTqO3ZmluzvNkthaw

# **First Code**
"""

pip install optuna

"""Cell 1 – Imports & Setup"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
import optuna

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
import warnings
warnings.filterwarnings("ignore")

"""Cell 2 – Load Data & Initial Cleaning"""

df = pd.read_csv('/content/sample_data/dengue data 17-07-2025.csv')
df.drop(columns=['city', 'week_start_date'], inplace=True)
df.fillna(method='ffill', inplace=True)
df.index

"""Cell 3 – Feature Engineering + Outlier Removal"""

# Outlier removal using IQR
def remove_outliers(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    return df[~((df < (Q1 - 1.5*IQR)) | (df > (Q3 + 1.5*IQR))).any(axis=1)]

df = remove_outliers(df)

# Add lag features
for lag in [1, 2, 3]:
    df[f'total_cases_lag{lag}'] = df['total_cases'].shift(lag)
df.dropna(inplace=True)

# # Outlier removal using IQR
# def remove_outliers(df):
#     Q1 = df.quantile(0.25)
#     Q3 = df.quantile(0.75)
#     IQR = Q3 - Q1
#     return df[~((df < (Q1 - 1.5*IQR)) | (df > (Q3 + 1.5*IQR))).any(axis=1)]

# df = remove_outliers(df)

# # Add lag features
# for lag in [1, 2, 3]:
#     df[f'total_cases_lag{lag}'] = df['total_cases'].shift(lag)
# df['month'] = pd.to_datetime(df['week_start_date'], format='%Y-%m-%d').dt.month
# df.dropna(inplace=True)

"""Cell 4 – Feature Filtering and Scaling"""

# Target column
target_col = 'total_cases'
features = df.drop(columns=[target_col])

# Remove low variance & high correlation
def drop_low_variance(df, thresh=0.01):
    sel = VarianceThreshold(thresh)
    return df[df.columns[sel.fit(df).get_support()]]
def drop_high_corr(df, thresh=0.9):
    corr = df.corr().abs()
    up = corr.where(np.triu(np.ones(corr.shape),1).astype(bool))
    drop = [c for c in up.columns if any(up[c] > thresh)]
    return df.drop(columns=drop)

features = drop_low_variance(features)
features = drop_high_corr(features)

# Scale features & target
feat_scaler = MinMaxScaler()
target_scaler = MinMaxScaler()
X_feat = feat_scaler.fit_transform(features)
y = df[target_col].values.reshape(-1,1)
y_scaled = target_scaler.fit_transform(y)

# # Target column
# target_col = 'total_cases'
# features = df.drop(columns=[target_col, 'week_start_date'])

# # Remove low variance & high correlation
# def drop_low_variance(df, thresh=0.01):
#     sel = VarianceThreshold(thresh)
#     return df[df.columns[sel.fit(df).get_support()]]
# def drop_high_corr(df, thresh=0.9):
#     corr = df.corr().abs()
#     up = corr.where(np.triu(np.ones(corr.shape),1).astype(bool))
#     drop = [c for c in up.columns if any(up[c] > thresh)]
#     return df.drop(columns=drop)

# features = drop_low_variance(features)
# features = drop_high_corr(features)

# # Scale features & target
# feat_scaler = MinMaxScaler()
# target_scaler = MinMaxScaler()
# X_feat = feat_scaler.fit_transform(features)
# y = df[target_col].values.reshape(-1,1)
# y_scaled = target_scaler.fit_transform(y)

"""Cell 5 – Sequence Creation & Split"""

window = 10
X, y_seq = [], []
for i in range(len(y_scaled)-window):
    X.append(X_feat[i:i+window])
    y_seq.append(y_scaled[i+window])
X, y_seq = np.array(X), np.array(y_seq)

# Split: Train 70%, Val 10%, Test 20%
idx = len(X)
test_size = int(0.2*idx)
val_size = int(0.1*idx)

X_train, X_val, X_test = X[:-test_size-val_size], X[-test_size-val_size:-test_size], X[-test_size:]
y_train, y_val, y_test = y_seq[:-test_size-val_size], y_seq[-test_size-val_size:-test_size], y_seq[-test_size:]

"""Cell 6 – Optuna Tuning"""

def create_model(input_shape, trial):
    model = Sequential()
    n = trial.suggest_int('neurons', 64, 256, step=32)
    model.add(LSTM(n, input_shape=input_shape, return_sequences=True))
    model.add(Dropout(trial.suggest_uniform('drop',0.0,0.5)))
    model.add(LSTM(n//2))
    model.add(Dropout(trial.suggest_uniform('drop2',0.0,0.5)))
    model.add(Dense(1))
    opt = trial.suggest_categorical('opt', ['adam','nadam','rmsprop'])
    model.compile(optimizer=opt, loss='mse')
    return model

def objective(trial):
    model = create_model((window, X.shape[2]), trial)
    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    model.fit(X_train, y_train, epochs=100, batch_size=trial.suggest_categorical('bs',[16,32,64]),
              validation_data=(X_val,y_val), callbacks=[es], verbose=0)
    pred = model.predict(X_val)
    return mean_squared_error(y_val, pred)

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

print("Best hyperparameters:", study.best_trial.params)

"""Cell 7 – Final Model Training"""

p = study.best_trial.params
model = Sequential([
    LSTM(p['neurons'], return_sequences=True, input_shape=(window,X.shape[2])),
    Dropout(p['drop']),
    LSTM(p['neurons']//2),
    Dropout(p['drop2']),
    Dense(1)
])
model.compile(optimizer=p['opt'], loss='mse')
es = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)
model.fit(X_train, y_train, epochs=150, batch_size=p['bs'],
          validation_data=(X_val,y_val), callbacks=[es], verbose=4)

"""Cell 8 – Predict & Inverse Scale"""

pred_scaled = model.predict(X_test)
pred = target_scaler.inverse_transform(pred_scaled)
actual = target_scaler.inverse_transform(y_test)

mse = mean_squared_error(actual, pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(actual, pred)
print(f"MSE: {mse:.2f}, RMSE: {rmse:.2f}, MAE: {mae:.2f}")

"""Cell 9 – Plot Predictions vs Actual"""

plt.figure(figsize=(12,6))
plt.plot(actual, label='Actual Cases')
plt.plot(pred, label='Predicted Cases')
plt.title('Dengue Case Prediction (Test Set)')
plt.xlabel('Time Step')
plt.ylabel('Total Cases')
plt.legend()
plt.grid(True)
plt.show()



"""# **Second Code**"""

pip install optuna

"""Cell 1: Imports & Setup"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
import optuna

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping

import warnings
warnings.filterwarnings("ignore")

"""Cell 2: Load Data & Initial Cleaning"""

df = pd.read_csv('/content/sample_data/dengue data 17-07-2025.csv')

# Keep 'week_start_date' for feature engineering but drop 'city'
df.drop(columns=['city'], inplace=True)
df.fillna(method='ffill', inplace=True)

"""Cell 3: Remove Outliers & Feature Engineering"""

# Remove outliers based on IQR for all numeric columns except date and target
def remove_outliers(df, target_col='total_cases'):
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    numeric_cols.remove(target_col)
    Q1 = df[numeric_cols].quantile(0.25)
    Q3 = df[numeric_cols].quantile(0.75)
    IQR = Q3 - Q1
    filter_ = ~((df[numeric_cols] < (Q1 - 1.5 * IQR)) | (df[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)
    return df[filter_]

df = remove_outliers(df)

# Extract month from 'week_start_date'
df['month'] = pd.to_datetime(df['week_start_date']).dt.month

# Create lag features for 'total_cases' (lags 1 to 6)
for lag in range(1, 7):
    df[f'total_cases_lag_{lag}'] = df['total_cases'].shift(lag)

# Rolling mean and std dev of 'total_cases' over last 3 weeks
df['rolling_mean_3'] = df['total_cases'].rolling(window=3).mean()
df['rolling_std_3'] = df['total_cases'].rolling(window=3).std()

# Drop rows with NaNs due to lagging
df.dropna(inplace=True)

# Drop date column (used month already)
df.drop(columns=['week_start_date'], inplace=True)

"""Cell 4: Remove Low-Variance & Highly Correlated Features"""

target_col = 'total_cases'

# Separate features and target
features = df.drop(columns=[target_col])
target = df[target_col]

# Remove low variance features
selector = VarianceThreshold(threshold=0.01)
features = features.loc[:, selector.fit(features).get_support()]

# Remove highly correlated features (threshold 0.9)
corr_matrix = features.corr().abs()
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]
features.drop(columns=to_drop, inplace=True)

print("Remaining features after variance & correlation filtering:\n", features.columns)

"""Cell 5: Scaling Features and Target"""

feat_scaler = MinMaxScaler()
target_scaler = MinMaxScaler()

features_scaled = feat_scaler.fit_transform(features)
target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))

"""Cell 6: Create Sequences for LSTM & Train-Val-Test Split"""

window_size = 10
X, y = [], []

for i in range(len(features_scaled) - window_size):
    X.append(features_scaled[i:i+window_size])
    y.append(target_scaled[i+window_size])

X = np.array(X)
y = np.array(y)

# Split data: 70% train, 15% val, 15% test (time series split - no shuffle)
total_len = len(X)
train_len = int(0.7 * total_len)
val_len = int(0.15 * total_len)

X_train, y_train = X[:train_len], y[:train_len]
X_val, y_val = X[train_len:train_len + val_len], y[train_len:train_len + val_len]
X_test, y_test = X[train_len + val_len:], y[train_len + val_len:]

"""Cell 7: Define Model and Objective for Optuna Hyperparameter Tuning"""

def create_model(input_shape, trial):
    model = Sequential()
    n_layers = trial.suggest_int('n_layers', 1, 3)
    for i in range(n_layers):
        units = trial.suggest_int(f'units_l{i}', 64, 256, step=32)
        return_seq = i < n_layers - 1
        if i == 0:
            model.add(LSTM(units, return_sequences=return_seq, input_shape=input_shape))
        else:
            model.add(LSTM(units, return_sequences=return_seq))
        model.add(Dropout(trial.suggest_float(f'dropout_l{i}', 0.1, 0.5)))
    model.add(Dense(1))
    optimizer = trial.suggest_categorical('optimizer', ['adam', 'nadam', 'rmsprop'])
    model.compile(optimizer=optimizer, loss='mse')
    return model

def objective(trial):
    model = create_model((window_size, X.shape[2]), trial)
    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100,
        batch_size=trial.suggest_categorical('batch_size', [16, 32, 64]),
        callbacks=[early_stop],
        verbose=0
    )
    pred_val = model.predict(X_val)
    val_mse = mean_squared_error(y_val, pred_val)
    return val_mse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

print("Best parameters:", study.best_trial.params)

"""Cell 8: Train Final Model with Best Hyperparameters"""

p = study.best_trial.params

model = Sequential()
for i in range(p['n_layers']):
    units = p[f'units_l{i}']
    return_seq = i < p['n_layers'] - 1
    if i == 0:
        model.add(LSTM(units, return_sequences=return_seq, input_shape=(window_size, X.shape[2])))
    else:
        model.add(LSTM(units, return_sequences=return_seq))
    model.add(Dropout(p[f'dropout_l{i}']))
model.add(Dense(1))
model.compile(optimizer=p['optimizer'], loss='mse')

early_stop = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=150,
    batch_size=p['batch_size'],
    callbacks=[early_stop],
    verbose=2
)

"""Cell 9: Predict on Test Set and Inverse Scale for Evaluation"""

pred_scaled = model.predict(X_test)
pred = target_scaler.inverse_transform(pred_scaled)
actual = target_scaler.inverse_transform(y_test)

mse = mean_squared_error(actual, pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(actual, pred)

print(f"Test MSE: {mse:.2f}")
print(f"Test RMSE: {rmse:.2f}")
print(f"Test MAE: {mae:.2f}")

"""Cell 10: Plot Actual vs Predicted Dengue Cases"""

plt.figure(figsize=(14, 6))
plt.plot(actual, label='Actual Cases')
plt.plot(pred, label='Predicted Cases')
plt.title("Dengue Case Prediction (Test Set)")
plt.xlabel("Time Step")
plt.ylabel("Total Cases")
plt.legend()
plt.grid(True)
plt.show()